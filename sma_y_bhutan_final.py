# -*- coding: utf-8 -*-
"""SMA_Y_Bhutan_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cv0Zvw5GEVFSa3n3mNFewY7VO4xyNCGu

# Git Clones
"""

!git clone https://github.com/dhruvgargindia/SMA_Y_Bhutan/

import os
os.chdir("SMA_Y_Bhutan")

"""# Helper functions"""

def get_completion(prompt, instructions, client, model="gpt-4-turbo",
                   output_type = 'text'):
  '''Get a text completion from the OpenAI API'''
  completion = client.chat.completions.create(
                model=model,
                response_format={ "type": output_type},
                messages=[
                  {"role": "system", "content": instructions},
                  {"role": "user", "content": prompt}
                ]
              )
  response =completion.choices[0].message.content

  return response

def generate_image(prompt = "Draw a cute bunny", model = "dall-e-3"):
  '''Generates an image using the OpenAI API'''

  response_img = client.images.generate(
    model= model,
    prompt=prompt,
    size="1024x1024",
    quality="standard",
    n=1,
  )
  time.sleep(1)
  image_url = response_img.data[0].url
  revised_prompt = response_img.data[0].revised_prompt

  return image_url, revised_prompt

def generate_image_description(image_urls, instructions):
  '''Generates a description of a list of image_urls using the OpenAI Vision API'''
  PROMPT_MESSAGES = [
    {
        "role": "user",
        "content": [{"type": "text","text":instructions},
            *map(lambda x: {"type":"image_url","image_url": x}, image_urls),
        ],
    },
  ]
  params = {
      "model": "gpt-4-vision-preview",
      "messages": PROMPT_MESSAGES,
      "max_tokens": 1000,
  }

  response= client.chat.completions.create(**params)


  image_description = response.choices[0].message.content
  return image_description

def encode_image(image_path):
    '''Encodes an image to base64'''
    # Open the image
    with Image.open(image_path) as img:
        # Resize the image if needed (optional)
        # Calculate the target size based on the percentage
        width, height = img.size
        target_width = int(width * (10 / 100))
        target_height = int(height * (10 / 100))

        # Resize the image to the target size
        img = img.resize((target_width, target_height))

        # Convert the image to JPEG format and encode it as base64
        buffer = io.BytesIO()
        img.convert("RGB").save(buffer, format="JPEG", quality=80)  # Adjust quality as needed
        return base64.b64encode(buffer.getvalue()).decode('utf-8')

def display_image_url(image_url, width = 500, height = 500):
  '''Display the image located at image_url so it remains in the notebook
  even after the link dies '''
  response = requests.get(image_url)
  image_data = response.content
  # Encoding the image data as base64
  base64_image = base64.b64encode(image_data).decode('utf-8')
  # Generating HTML to display the image
  html_code = f'<img src="data:image/jpeg;base64,{base64_image}" width="{width}" height="{height}"/>'
  # Displaying the image in the notebook
  display(HTML(html_code))
  return html_code


def display_IG(caption, image_url, screen_name=None, profile_image_url = None):
    ''' HTML template for displaying the image, screen name, and caption in an Instagram-like format'''

    display_html = f"""
    <style>
        .instagram-post {{
            border: 1px solid #e1e1e1;
            border-radius: 3px;
            width: 600px;
            margin: 20px auto;
            background-color: white;
            font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
        }}
        .instagram-header {{
            padding: 14px;
            border-bottom: 1px solid #e1e1e1;
            display: flex;
            align-items: center;
        }}
        .instagram-profile-pic {{
            border-radius: 50%;
            width: 32px;
            height: 32px;
            margin-right: 10px;
        }}
        .instagram-screen-name {{
            font-weight: bold;
            color: #262626;
            text-decoration: none;
            font-size: 14px;
        }}
        .instagram-image {{
            max-width: 600px;
            width: auto;
            height: auto;
            display: block;
            margin: auto;
        }}
        .instagram-caption {{
            padding: 10px;
            font-size: 14px;
            color: #262626;
        }}
        .instagram-footer {{
            padding: 10px;
            border-top: 1px solid #e1e1e1;
        }}
        .instagram-likes {{
            font-weight: bold;
            margin-bottom: 8px;
        }}
    </style>
    <div class="instagram-post">
        <div class="instagram-header">
            <img src="{profile_image_url}" alt="Profile picture" class="instagram-profile-pic">
            <a href="#" class="instagram-screen-name">{screen_name}</a>
        </div>
        <img src="{image_url}" alt="Instagram image" class="instagram-image">
        <div class="instagram-caption">
            <a href="#" class="instagram-screen-name">{screen_name}</a> {caption}
        </div>
        <div class="instagram-footer">
            <div class="instagram-likes">24 likes</div>
            <!-- Include other footer content here -->
        </div>
    </div>
    """
    display(HTML(display_html))
    return display_html

"""# Import and Open AI Stuff"""

#Install OpenAI
!pip install openai -q

from transformers import pipeline
import torch
from collections import Counter
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import json
import re
#import scripts.TextAnalysis as ta
from textwrap import fill

#progress bar for long computations
from tqdm import tqdm

pd.set_option("display.max_colwidth", None)
#this code makes the default font sizes big in plots
plt.rcParams.update({'axes.labelsize': 18,
                     'xtick.labelsize': 14,
                     'ytick.labelsize': 14})

import openai
os.environ['OPENAI_API_KEY'] = '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'

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

client = openai.Client(api_key=OPENAI_API_KEY)

"""# Load and Merge CSVs"""

# prompt: load data files in the FinalData folder and merge all 4 csv files into one dataframe and call it df

import pandas as pd

# Get a list of all CSV files in the FinalData folder
csv_files = [file for file in os.listdir("FinalData") if file.endswith(".csv")]

# Read each CSV file into a Pandas DataFrame
data_frames = [pd.read_csv(os.path.join("FinalData", file)) for file in csv_files]

# Merge all DataFrames into a single DataFrame
df = pd.concat(data_frames, ignore_index=True)

df.head()

# prompt: list all the different Post Owner in df and remove all the farmrio Post Owner rows from df

# List all the different Post Owner in df
post_owners = df['Post Owner'].unique()
print(post_owners)

# Remove all the farmrio Post Owner rows from df
df = df[df['Post Owner'] != 'farmrio']

# prompt: list all the different Post Owner in df

print(df['Post Owner'].unique().tolist())

# prompt: edit the dataframe df to remove "/workspaces/SMA_Y_Bhutan/" from each of the image link columns

df['Image Link 1'] = df['Image Link 1'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 2'] = df['Image Link 2'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 3'] = df['Image Link 3'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 4'] = df['Image Link 4'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 5'] = df['Image Link 5'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 6'] = df['Image Link 6'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 7'] = df['Image Link 7'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 8'] = df['Image Link 8'].str.replace('/workspaces/SMA_Y_Bhutan/', '')
df['Image Link 9'] = df['Image Link 9'].str.replace('/workspaces/SMA_Y_Bhutan/', '')

#df.head()

# Import the necessary modules
import base64
import io
from PIL import Image
from IPython.display import Image

# Path to your image
image_path = f"data/analysis/BODE IG Photos/3147582617079516544_17952249.jpg"

# Encode the image in base64
base64_image = encode_image(image_path)

# Create the image URL
image_url = f"data:image/jpeg;base64,{base64_image}"

# Display the image
display(Image(url=image_url, width=500))

"""# Basic Analysis

"""

# @title Post Owner vs Media Count

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(df['Post Owner'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df, x='Media Count', y='Post Owner', inner='box', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

# @title Post Owner vs Likes

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(df['Post Owner'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df, x='Likes', y='Post Owner', inner='box', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

plt.figure(figsize=(8,8))
sns.countplot(data=df, y ='Post Owner',
              hue='Post Owner',
              palette = "husl")
plt.ylabel("Post Owner", fontsize = 14)
plt.xlabel("Number of Comments", fontsize = 14)
plt.grid()
plt.show()

# prompt: add up all the likes for each post owner in the dataframe df

# Create a dictionary to store the total likes for each post owner
total_likes = {}

# Iterate through the rows of the DataFrame
for index, row in df.iterrows():
  # Get the post owner and the number of likes
  post_owner = row['Post Owner']
  likes = row['Likes']

  # If the post owner is not already in the dictionary, add it
  if post_owner not in total_likes:
    total_likes[post_owner] = 0

  # Add the number of likes to the total for that post owner
  total_likes[post_owner] += likes

# Print the total likes for each post owner
for post_owner, total_like in total_likes.items():
  print(f"{post_owner}: {total_like}")

# prompt: make a chart of the likes total above

import matplotlib.pyplot as plt
# Create a bar chart of the total likes for each post owner
plt.figure(figsize=(12, 6))
sns.barplot(x=list(total_likes.keys()), y=list(total_likes.values()))
plt.xlabel("Post Owner", fontsize=14)
plt.ylabel("Total Likes", fontsize=14)
plt.title("Total Likes for Each Post Owner", fontsize=16)
plt.show()

"""# Engagement Analysis

## Calculate Engagement
"""

follower_count ={'kelzangtextiles': 3650, 'bode':568000, 'norlha_atelier':84200}
print(follower_count)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import plotly.express as px
import openai
import requests
from PIL import Image
from io import BytesIO

# Convert Post Owner to String
df['Owner_string'] = df['Post Owner'].astype(str)

# Assuming you have a dictionary 'follower_count' mapping Instagram account names to their follower counts
# Normalize the 'Likes' column
df['Likes_Normalized'] = df['Likes'] / df['Owner_string'].map(follower_count)

# Normalize the 'Number of Comments' column
df['Comments_Normalized'] = df['Number of Comments'] / df['Owner_string'].map(follower_count)

# Calculate the total engagement score
df['Total_Engagement'] = df['Likes_Normalized'] + 10*df['Comments_Normalized']

# Print the first few rows of the updated DataFrame
df.head()

"""## Engagement vs Post Owner"""

# prompt: Calculate and plot total engagement against post owner

# Group the DataFrame by 'Post Owner' and calculate the sum of 'Total_Engagement'
engagement_by_owner = df.groupby('Post Owner')['Total_Engagement'].sum()

# Sort the engagement_by_owner Series in descending order
engagement_by_owner = engagement_by_owner.sort_values(ascending=False)

# Create a bar chart of the total engagement for each post owner
plt.figure(figsize=(12, 6))
sns.barplot(x=engagement_by_owner.index, y=engagement_by_owner.values)
plt.xlabel("Post Owner", fontsize=14)
plt.ylabel("Total Engagement", fontsize=14)
plt.title("Total Engagement for Each Post Owner", fontsize=16)
plt.show()

"""## Wordcloud Captions by Engagement

"""

# prompt: Generate wordcloud for high and low engagement captions for each post owner. First store the high and low engagement captions in qhigh and qlow dataframes.

# Calculate the quantiles for high and low engagement for each post owner
high_quantile = df.groupby('Post Owner')['Total_Engagement'].transform('quantile', 0.9)
low_quantile = df.groupby('Post Owner')['Total_Engagement'].transform('quantile', 0.1)

# Create a dataframe of high engagement captions for each post owner
qhigh = df[df['Total_Engagement'] >= high_quantile].groupby('Post Owner')['Caption'].apply(list)

# Create a dataframe of low engagement captions for each post owner
qlow = df[df['Total_Engagement'] <= low_quantile].groupby('Post Owner')['Caption'].apply(list)

# Create a function to generate a wordcloud for a list of captions
def generate_wordcloud(captions):
  # Join all captions into a single string
  text = " ".join(captions)

  # Create a wordcloud object
  wordcloud = WordCloud(stopwords=STOPWORDS, background_color="white").generate(text)

  # Plot the wordcloud
  plt.figure(figsize=(10, 8))
  plt.imshow(wordcloud, interpolation="bilinear")
  plt.axis("off")
  plt.show()

# Generate wordclouds for high engagement captions
for post_owner, captions in qhigh.items():
  print(f"High engagement wordcloud for {post_owner}:")
  generate_wordcloud(captions)

# Generate wordclouds for low engagement captions
for post_owner, captions in qlow.items():
  print(f"Low engagement wordcloud for {post_owner}:")
  generate_wordcloud(captions)

"""## Summarize High and Low Engagement Captions"""

# prompt: Take the top 5 high engagement captions for each post owner from the qhigh dataframe. Then combine them and pass all of them at once to OpenAI API using instructions and prompts in the get_completions function to  find what makes them highly engaging

# Get the top 5 high engagement captions for each post owner
top_5_high_engagement_captions = {}
for post_owner, captions in qhigh.items():
    sorted_captions = sorted(captions, key=len, reverse=True)  # Sort captions by length (optional)
    top_5_high_engagement_captions[post_owner] = sorted_captions[:5]

# Combine all top 5 high engagement captions into a single list
all_top_5_high_engagement_captions = []
for captions in top_5_high_engagement_captions.values():
    all_top_5_high_engagement_captions.extend(captions)

# Pass all high engagement captions to OpenAI API
instructions = f"""I am analyzing Instagram captions of 3 different accounts that
sell traditional or artisanal textile to see what makes a high engagement caption.
These are the top 5 high engagement captions for each post owner.
I need you to identify the common factors that makes them highly engaging.
Be specific"""
prompt = "\n".join(all_top_5_high_engagement_captions)
engagagement_factors = get_completion(prompt, instructions, client)

# Print the response
print(engagagement_factors)

"""# Image Analysis

"""

!pip install umap-learn -q

import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import numpy as np
import math
import json, re, os
import random

#Instagram analysis
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from PIL import Image
from PIL import Image as PILImage
from matplotlib.patches import Ellipse

#Image processing with Resnet
import torch
from torchvision import models, transforms
from sklearn.preprocessing import StandardScaler

#Low dimensional embedding algorithms
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from umap import UMAP

#Clustering algorithms
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

#display settings for figures
pd.set_option("display.max_colwidth", None)
plt.rcParams.update({'axes.labelsize': 18,
                     'xtick.labelsize': 14,
                     'ytick.labelsize': 14})
plt.rcParams['figure.figsize'] = [8, 6]

"""## Resnet 50 Model"""

import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using GPU:", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print("Using CPU")

# Load the pretrained ResNet model
resnet = models.resnet50(pretrained=True)
resnet.to(device)
# We're only interested in the embedding, so let's remove the final layer
resnet.fc = torch.nn.Identity()

# Ensure the model is in evaluation mode
resnet.eval()

# Define image transformations for neural network
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

"""## Helper Functions to compute embeddings"""

# Function to compute embedding vector for all images in file_list
def embed_images(file_list):
    features = []
    for cnt,img_file in enumerate(file_list):
        if cnt % 100 == 0:
            print(f"{cnt}/{len(file_list)} images processed..")
        if img_file.endswith(('png', 'jpg', 'jpeg')):
            img = Image.open(img_file).convert('RGB')
            img_t = transform(img)
            batch_t = torch.unsqueeze(img_t, 0)
            batch_t = batch_t.to(device)
            with torch.no_grad():
                out = resnet(batch_t)
            features.append(out.cpu().flatten().numpy())
    return np.array(features)

# Function to load image from path and downsample it so plots are faster
def loadImage(path, max_size = 100):
    """Resize the image to a maximum dimension of max_size and apply zoom."""
    with PILImage.open(path) as img:
        # Calculate new size, maintaining aspect ratio
        aspect_ratio = img.width / img.height
        if aspect_ratio > 1:  # Width > height
            new_size = (int(max_size * aspect_ratio), max_size)
        else:
            new_size = (max_size, int(max_size / aspect_ratio))
        # Resize image
        img_resized = img.resize(new_size, PILImage.LANCZOS)
        # Convert to array for plt
        img_array = np.array(img_resized)
        # Apply zoom
        return img_array

"""##Load Image paths for various owners

Load Image Paths for BODE IG Photos in **df_bode**
"""

# Path to your image folder
image_folder = '/content/SMA_Y_Bhutan/data/analysis/BODE IG Photos'

image_paths = [os.path.join(image_folder, img_file) for img_file in os.listdir(image_folder) if img_file.endswith(('png', 'jpg', 'jpeg'))]

# Function to extract the large number at the beginning of the filename
def extract_number(filepath):
    # Extract the filename from the filepath
    filename = os.path.basename(filepath)
    # Match the pattern: the number at the beginning of the filename before the first underscore
    match = re.search(r'^(\d+)_', filename)
    if match:
        return int(match.group(1))
    return 0  # Return 0 or some default value if no number is found

# Sort the list by the extracted number
image_paths.sort(key=lambda x: extract_number(x))


print(image_folder)
print(f"{len(image_paths)} images")

#dataframe with image info.
df_bode = pd.DataFrame({'image_path':image_paths})

"""Load Image Paths for Kelzand Handicraft IG Photos in **df_kel**"""

# Path to your image folder
image_folder = '/content/SMA_Y_Bhutan/data/analysis/Kelzang Handicraft IG Photos'

image_paths = [os.path.join(image_folder, img_file) for img_file in os.listdir(image_folder) if img_file.endswith(('png', 'jpg', 'jpeg'))]

# Function to extract the large number at the beginning of the filename
def extract_number(filepath):
    # Extract the filename from the filepath
    filename = os.path.basename(filepath)
    # Match the pattern: the number at the beginning of the filename before the first underscore
    match = re.search(r'^(\d+)_', filename)
    if match:
        return int(match.group(1))
    return 0  # Return 0 or some default value if no number is found

# Sort the list by the extracted number
image_paths.sort(key=lambda x: extract_number(x))


print(image_folder)
print(f"{len(image_paths)} images")

#dataframe with image info.
df_kel = pd.DataFrame({'image_path':image_paths})

"""Load Image Paths for Norla Atelier IG Photos in **df_norla**"""

# Path to your image folder
image_folder = '/content/SMA_Y_Bhutan/data/analysis/Norla Atelier IG Photos'

image_paths = [os.path.join(image_folder, img_file) for img_file in os.listdir(image_folder) if img_file.endswith(('png', 'jpg', 'jpeg'))]

# Function to extract the large number at the beginning of the filename
def extract_number(filepath):
    # Extract the filename from the filepath
    filename = os.path.basename(filepath)
    # Match the pattern: the number at the beginning of the filename before the first underscore
    match = re.search(r'^(\d+)_', filename)
    if match:
        return int(match.group(1))
    return 0  # Return 0 or some default value if no number is found

# Sort the list by the extracted number
image_paths.sort(key=lambda x: extract_number(x))


print(image_folder)
print(f"{len(image_paths)} images")

#dataframe with image info.
df_norla = pd.DataFrame({'image_path':image_paths})

# prompt: combine df_norla, df_kel, df_farmrio, and df_bode into df_all_images

import pandas as pd
df_all_images = pd.concat([df_norla, df_kel, df_bode], ignore_index=True)

df_all_images.head()

"""## Compute Image Embedding

"""

# prompt: We will compute the embedding of the images in df_all_images using the ResNet50 neural network. We do this using the embed_images function. We save the embedding vectors resnet_embedding

resnet_embedding = embed_images(df_all_images['image_path'].values)

"""###PCA Embeddings"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# pca = PCA(n_components=2)
# pca_embedding = pca.fit_transform(resnet_embedding)

"""###t-SNE Embeddings"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tsne = TSNE(n_components=2,
#             learning_rate='auto',
#             init='pca')
# tsne_embedding = tsne.fit_transform(resnet_embedding)

"""###UMAP Embeddings"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# umap = UMAP(n_neighbors=10,
#             min_dist=0.1,
#             metric='euclidean')
# umap_embedding = umap.fit_transform(resnet_embedding)

df_all_images['pca_x'] = pca_embedding[:,0]
df_all_images['pca_y'] = pca_embedding[:,1]
df_all_images['tsne_x'] = tsne_embedding[:,0]
df_all_images['tsne_y'] = tsne_embedding[:,1]
df_all_images['umap_x'] = umap_embedding[:,0]
df_all_images['umap_y'] = umap_embedding[:,1]

df_all_images.to_csv('/content/SMA_Y_Bhutan/df_all_images_embeddings', index = False)

"""###PCA Analysis"""

# Range of components to consider
n_components = 200

pca = PCA(n_components=n_components)
pca.fit(resnet_embedding)
explained_variances = np.cumsum(pca.explained_variance_ratio_)

# Plotting the explained variance
plt.figure(figsize=(10, 6))
plt.plot(range(1,n_components+1), explained_variances,
         marker='o')
plt.title('Explained Variance by Number of Components')
plt.xlabel('Number of PCA Components')
plt.ylabel('Total Explained Variance')
plt.ylim(0,1)
plt.grid()
plt.show()

"""## Plot Images as per embeddings"""

#this plots mini versions of the images in a scatter plot (uses a lot of RAM, so be careful)
def plot_image_embedding(df_all_images, col_x,col_y,
                         image_zoom = 0.3,
                         max_size = 100):

  x = df_all_images[col_x]
  y = df_all_images[col_y]
  image_paths = df_all_images['image_path']

  fig, ax = plt.subplots(figsize = (10,10))
  ax.scatter(x, y)  # Plotting just for the base scatter plot

  for x0, y0, path in zip(x, y, image_paths):
      img = loadImage(path, max_size)
      ab = AnnotationBbox(OffsetImage(img,
                                      zoom = image_zoom),
                          (x0, y0),
                          frameon=False)
      ax.add_artist(ab)

  return ax

df_all_images.head()

"""**Remove FarmRio IG Photos from the embeddings and from df_all_images**"""

# prompt: from df_all_images remove all rows where the text in "image_path" has the text "FarmRio IG Photos"

df_all_images = df_all_images[~df_all_images['image_path'].str.contains('FarmRio IG Photos')]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# embedding_names = ['pca','tsne','umap']
# image_zoom = 0.2
# for embedding_name in embedding_names:
#   col_x, col_y = f'{embedding_name}_x', f'{embedding_name}_y'
#   ax = plot_image_embedding(df_all_images, col_x,col_y, image_zoom)
#   ax.set_title(embedding_name)
#   plt.show()

"""## K-means Cluster

###Learn Optimal Number of Clusters
"""

# Choose your embedding to cluster
embedding_name = 'umap'
embedding = df_all_images[[f'{embedding_name}_x', f'{embedding_name}_y']].to_numpy()

# Define the range of K values to try
K_range = range(2, 30)  # For example, trying K from 2 to 10

# Prepare a list to store silhouette scores
silhouette_scores = []

# Compute KMeans and silhouette score for each K
for K in K_range:
    kmeans = KMeans(n_clusters=K,
                    random_state=42,
                    n_init='auto')
    kmeans.fit(embedding)
    labels = kmeans.labels_
    silhouette_avg = silhouette_score(embedding, labels)
    silhouette_scores.append(silhouette_avg)
    print(f"K = {K}, silhouette score = {silhouette_avg}")

# Find the optimal K (number of clusters) with the highest silhouette score
optimal_K = K_range[silhouette_scores.index(max(silhouette_scores))]
print(f"The silhouette optimal number of clusters (K) is: {optimal_K}")

# Plot silhouette scores over K
plt.figure(figsize=(10, 3))
plt.plot(K_range, silhouette_scores, marker='o')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Silhouette Score')
plt.grid()
plt.show()

"""### Cluster into optimal number of clusters"""

#we will use k=5 for the analysis
optimal_K = 3
kmeans = KMeans(n_clusters=optimal_K,
                random_state=42,
                n_init='auto')
kmeans.fit(embedding)
kmeans_labels = [str(x) for x in kmeans.labels_]
df_all_images['kmeans_label'] = kmeans_labels

# Get the centroids and labels
kmeans_centroids = kmeans.cluster_centers_
kmeans_labels = kmeans.labels_

# Save centroids and labels to files
np.save('kmeans_centroids.npy', kmeans_centroids)
np.save('kmeans_labels.npy', kmeans_labels)

sns.countplot(data = df_all_images, x = 'kmeans_label')
plt.grid()

sns.scatterplot(data = df_all_images ,
                x='umap_x',
                y = 'umap_y',
                hue = 'kmeans_label')
plt.grid()
plt.show()

"""###Plot Sample Images from Clusters and  generate cluster descriptions using OpenAI APIs"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import textwrap as tr
# import base64
# from IPython.display import display, Image, HTML, Audio
# from PIL import Image
# 
# nsamples = 12  #numer of samples per cluster
# max_size = 150  #width of image
# ncols = 4  #number of columsn in image grid
# 
# clusters = np.sort(df_all_images['kmeans_label'].unique())
# instructions = '''Describe the common theme in this cluster of images in 500 characters.'''
# cluster_image_descriptions = {}
# 
# for cluster in clusters:
#   df_cluster = df_all_images[df_all_images['kmeans_label'] == cluster]
#   print(f"\n\nCluster {cluster}: has {len(df_cluster)} images")
#       # Calculate the number of rows needed for the nx4 grid
# 
#   nrows = math.ceil(nsamples / ncols)
# 
#   # Create a figure with subplots
#   fig, axs = plt.subplots(nrows=nrows, ncols=ncols,
#                           figsize=(9, 9))
#   fig.suptitle(f'Cluster {cluster}', fontsize=16)
# 
#   # Flatten the axis array for easy indexing
#   axs = axs.ravel()
# 
#   # Hide unused subplots if nsamples is not a multiple of 4
#   for i in range(nsamples, len(axs)):
#       axs[i].axis('off')
# 
#   # Plot each sample
#   for idx, (i, row) in enumerate(df_cluster.iterrows()):
#       if idx >= nsamples:
#           break
#       img_path = row['image_path']
#       #img = Image.open(img_path)
#       img = loadImage(img_path, max_size)
#       axs[idx].imshow(img)
#       axs[idx].set_title(f"Sample {i}", fontsize=18)
#       axs[idx].axis('off')  # Hide the axis
# 
#   plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout
#   plt.show()
# 
#   # Sample some rows from the cluster
#   df_cluster_sample = df_all_images[df_all_images['kmeans_label'] == cluster].sample(nsamples)
#   # Get the image_path from the sampled rows
#   image_paths = df_cluster_sample['image_path'].tolist()
# 
#   # Convert the image_path into a URL that Vision can understand
#   # We encode the image as a giant byte stream and append that to the URL
#   image_urls = []
#   for image_path in image_paths:
#       base64_image = encode_image(image_path)
#       image_url = f"data:image/jpeg;base64,{base64_image}"
#       image_urls.append(image_url)
# 
#   # Display the sampled images
#   #for image_url in image_urls:
#   #    display(Image(url=image_url, width=500))
# 
#   # Generate image description
#   image_description = generate_image_description(image_urls, instructions)
#   cluster_image_descriptions[cluster] = image_description
#   print(tr.fill(image_description))
#   print("\n")

"""# Caption Generation

##Load Embeddings
"""

df = pd.read_csv('/content/SMA_Y_Bhutan/df_all_images_embeddings (2)')
df.head()

"""## Use OpenAI API to identify image cluster for new image




"""

import base64
import requests
from IPython.display import HTML, display
import time
from PIL import Image
import io

new_image = "/content/SMA_Y_Bhutan/data/analysis/ybhutan_photoshoot_in_bhutan/10A.JPG"
# Encode the new image to base64
new_image_base64 = encode_image(new_image)

# Combine the new image and cluster descriptions as instructions
instructions = f"""Classify the new image based on the following cluster descriptions. Return the output in the following format:
<Cluster number>:<Cluster description>"""
for cluster, description in cluster_image_descriptions.items():
  instructions += f"\nCluster {cluster}: {description}"

  # Use OpenAI Vision API to classify the new image based on instructions
  classification_result = get_completion(new_image_base64, instructions, client, model="gpt-4-turbo", output_type='text')


print("Classification result:")
print(classification_result)

import os
import pandas as pd

# Initialize an empty DataFrame to store the results
df = pd.DataFrame(columns=["Filename", "Classification Result"])

# Get a list of all files in the directory
files = os.listdir("/content/SMA_Y_Bhutan/data/analysis/ybhutan_photoshoot_in_bhutan")

# Filter the list to only include files that contain the letter "A"
files_with_a = [f for f in files if "A" in f]

# Classify each image with the letter "A" in the filename
for filename in files_with_a:
    # Get the full path of the image
    image_path = os.path.join("/content/SMA_Y_Bhutan/data/analysis/ybhutan_photoshoot_in_bhutan", filename)

    # Encode the image to base64
    image_base64 = encode_image(image_path)

    # Combine the new image and cluster descriptions as instructions
    instructions = f"""Classify the new image based on the following cluster descriptions. Return the output in the following format:
    <Cluster number>:<Cluster description>"""
    for cluster, description in cluster_image_descriptions.items():
        instructions += f"\nCluster {cluster}: {description}"

    # Use OpenAI Vision API to classify the new image based on instructions
    classification_result = get_completion(image_base64, instructions, client, model="gpt-4-vision-preview", output_type='text')

    # Append the classification result to the DataFrame
    df = pd.concat([df, pd.DataFrame({"Filename": filename, "Classification Result": classification_result}, index=[0])], ignore_index=True)

# Print the DataFrame
df.head()

"""## Generate Caption using Cluster description and high engagement captions"""

# prompt: now find the instagram_descriptions for each image in df based on the classification result and image in "FileName". there are 18 images. append the instagram_descriptions to df

# Initialize an empty list to store the generated Instagram descriptions
instagram_descriptions = []

# Loop through each row in the DataFrame
for index, row in df.iterrows():
    # Get the filename of the image
    filename = row["Filename"]

    # Get the classification result for the image
    classification_result = row["Classification Result"]

    # Get the image path
    image_path = os.path.join("/content/SMA_Y_Bhutan/data/analysis/ybhutan_photoshoot_in_bhutan", filename)

    # Encode the image to base64
    image_base64 = encode_image(image_path)

    page_description = f""" You are the brand manager for Y Bhutan, an innovative brand dedicated to importing authentic Bhutanese products,
    such as handwoven jackets and purses made from yak wool and silk. Y Bhutan not only symbolizes peace, strength, gratitude but also
    represents a social impact venture where the majority of proceeds are reinvested into Bhutanese communities,
    supporting the country's unique measure of progress through Gross National Happiness.
    Y Bhutan aims to influence the high-end fashion and art scene by introducing products that tell a story of empowerment,
    sustainability, and cultural richness.
  Please use the following guidelines to support our launch of the Bhutan Believe Collection:
  - The kingdom of Bhutan is steeped in history, but our gaze is fixed on the future. This is our moment of evolution.
  Guardians of some of the world’s most pristine, wild and sacred places –
  and of a rich, deeply rooted culture – we are steadfast as the cypress in our commitment to conservation.
  Our future requires us to protect our heritage and to forge fresh pathways for forthcoming generations.
  Those who seek us out are called here. Arriving as guests, you become our partners in this transformative moment
  and make a meaningful contribution towards preserving what is priceless. We see a bright future.
  And we believe in our ability and responsibility to realize it together, and shine as a beacon of possibility in the world.
  - BELIEVE IN THE FUTURE; BELIEVE IN OURSELVES; BELIEVE IN ELEVATED VALUES; BELIEVE IN OUR WORTH; BELIEVE IN NATURE

Currently, Y Bhutan is launching their ‘Discover Bhutan’ series. The Discover Bhutan series dives into the enchanting beauty of Bhutan
through our posts featuring students from the second annual International Experience: Bhutan hosted by the Yale School of Management. 📚🇧🇹
Each snapshot captures the unique blend of stunning landscapes and impressive architecture witnessed during their spring break journey,
complementing their beautiful Yathra jackets. The instagram_descriptions you generate will be part of the Discover Bhutan series"""

    # Combine the new image and cluster descriptions as instructions
    instructions = f"""You are generating captions for an instagram page called Y Bhutan with the following description:\n
    {page_description}\n
    A new image was classified based on the following cluster description:\n
    {classification_result}\n
    Generate a captivating Instagram description for this image based on the above description.
    A captivating instagram caption has the following factors -
    {engagagement_factors}\n
    It is not a giveaway but just a publicity post\nT"""

    # Call the generate_image_description function with the new image's base64 string and instructions
    instagram_description = get_completion(image_base64, instructions, client, model="gpt-4-turbo", output_type='text')

    # Append the Instagram description to the list
    instagram_descriptions.append(instagram_description)

# Add the list of Instagram descriptions to the DataFrame
df["Instagram Description"] = instagram_descriptions

# Print the DataFrame
df.head()

# prompt: save the df to SMA_Y_Bhutan as a csv

df.to_csv('/content/SMA_Y_Bhutan/FINAL_images_with_captions.csv', index = False)